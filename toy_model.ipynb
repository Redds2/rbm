{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3585c5a8",
   "metadata": {},
   "source": [
    "energy and cost function which we're going to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64b3e2",
   "metadata": {},
   "source": [
    "$$ E(v,h) = - \\Sigma_i a_i v_i - \\Sigma_j b_j h_j - \\Sigma_{i,j} w_{ij} v_i h_j $$\n",
    "$$ C = -\\Sigma_v p_{target} (v) \\ln p(v) = -\\Sigma_v \\frac{1}{Z} p_{target} (v) e^{-E(v,h)} =  -\\Sigma_v \\frac{1}{Z} p_{target} (v) e^{\\Sigma_i a_i v_i + \\Sigma_j b_j h_j + \\Sigma_{i,j} w_{ij} v_i h_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07817f5",
   "metadata": {},
   "source": [
    "let's calculate explicitly gradient of cost function with respect to parameters $a_i, b_j, w_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a2cb2",
   "metadata": {},
   "source": [
    "#### wrt $a_i$\n",
    "$$ \\frac{\\partial}{\\partial a_i}\\ln p(v) = \\frac{ \\frac{\\partial}{\\partial a_i} \\Sigma_h p(v,h) }{\\Sigma_h p(v,h)} = \\\\ \\frac{ \\frac{\\partial}{\\partial a_i} [\\frac{1}{Z} \\Sigma_h  e^{-E(v,h)} ] Z }{ \\Sigma_h e^{-E(v,h)}} =  \\frac{ \\frac{1}{Z} Z \\frac{\\partial}{\\partial a_i} \\Sigma_h e^{-E(v,h)} }{\\Sigma_h e^{-E(v,h)}} + \\frac{ \\Sigma_h e^{-E(v,h)} Z \\frac{\\partial}{\\partial a_i} \\frac{1}{Z} }{\\Sigma_h  e^{-E(v,h)} } = \\\\ \\frac{ \\frac{\\partial}{\\partial a_i} \\Sigma_h e^{-E(v,h)} } {\\Sigma_h e^{-E(v,h)}} - Z \\frac{1}{Z^2} \\frac{\\partial}{\\partial a_i}Z = \\\\ \\frac{ \\frac{\\partial}{\\partial a_i} \\Sigma_h e^{-E(v,h)} } {\\Sigma_h e^{-E(v,h)}} - \\frac{1} { \\Sigma_{v}\\Sigma_{h} e^{-E(v,h)} } \\frac{\\partial}{\\partial a_i} \\Sigma_{v} \\Sigma_{h}  e^{-E(v,h)} = \\frac{ v_i  \\Sigma_h e^{-E(v,h)} } { {\\Sigma_h e^{-E(v,h)}} } - \\frac{1} { \\Sigma_{v}\\Sigma_{h} e^{-E(v,h)} } \\Sigma_{v} \\Sigma_{h} v_i e^{-E(v,h)} =  v_i - \\frac{\\Sigma_{v} \\Sigma_{h}v_i  e^{-E(v,h)}} { \\Sigma_{v}\\Sigma_{h} e^{-E(v,h)} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5b88f",
   "metadata": {},
   "source": [
    "#### wrt $w_{ij}$\n",
    "$$ \\frac{\\partial}{\\partial w_{ij} }\\ln p(v) = \\frac{ \\Sigma_h v_i h_j e^{-E(v,h)} }{\\Sigma_h e^{-E(v,h)}} - \\frac{1} { \\Sigma_{v}\\Sigma_{h} e^{-E(v,h)} } \\Sigma_{v} \\Sigma_{h} v_i h_j e^{-E(v,h)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef323f9c",
   "metadata": {},
   "source": [
    "#### wrt $b_i$\n",
    "$$ \\frac{\\partial}{\\partial b_i}\\ln p(v) = \\frac{ \\Sigma_h h_i e^{-E(v,h)} } { {\\Sigma_h e^{-E(v,h)}} } - \\frac{1} { \\Sigma_{v}\\Sigma_{h} e^{-E(v,h)} } \\Sigma_{v} \\Sigma_{h}h_i e^{-E(v,h)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17f50e",
   "metadata": {},
   "source": [
    "let's consider the simplest possible case with two neurons (1 hidden and 1 visible) and try to derive explicit expressions for coefficients values that minimize the cross-entropy. since we have only one neuron of each type and one connection we label the corresponding weight simply as $a$,$b$ and $w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb54979",
   "metadata": {},
   "source": [
    "#### auxiliary calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d92c1",
   "metadata": {},
   "source": [
    "$$ Z = \\Sigma_{v}\\Sigma_{h} e^{-E(v,h)} = \\Sigma_{v}\\Sigma_{h} e^{av + bh + wvh} = \\Sigma_h e^{bh} + e^{a + bh +wh} = 1 + e^b + e^a + e^{a+b+w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1786a2",
   "metadata": {},
   "source": [
    "$$ \\Sigma_{h} e^{-E(v,h)} = e^{av} + e^{av + b + wv}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652237c4",
   "metadata": {},
   "source": [
    "$$ \\Sigma_{h} h e^{-E(v,h)} = e^{av + b + wv}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c08f5d",
   "metadata": {},
   "source": [
    "$$ \\Sigma_{h} v e^{-E(v,h)} =v (e^{av} + e^{av + b + wv})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095d17e",
   "metadata": {},
   "source": [
    "$$ \\Sigma_{v} \\Sigma_{h}v  e^{-E(v,h)} = e^a (1+e^{b+w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e553bc",
   "metadata": {},
   "source": [
    "$$  \\Sigma_{v} \\Sigma_{h} v h e^{-E(v,h)} = e^{a+b+w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f1f4e",
   "metadata": {},
   "source": [
    "#### the first extremum condition is\n",
    "$$ \\frac{\\partial}{\\partial a} C = -\\Sigma_v p_{target}(v) \\frac{ \\partial}{\\partial a} \\ln p(v) = -[ p_0 (-\\frac{e^a (1+e^{b+w})}{1 + e^b + e^a + e^{a+b+w}}) + p_1 (1-\\frac{e^a (1+e^{b+w})}{1 + e^b + e^a + e^{a+b+w}}) ] = p_0 A - p_1 (1-A) = 0 $$\n",
    "where we denote probability of v being in state 0 and 1 as p_0 and p_1. since there's only one neuron and two possible states $$p_1 := p; p_0 = 1-p$$\n",
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575858ef",
   "metadata": {},
   "source": [
    " $$ (1-p)A - p(1-A) = 0 $$\n",
    " $$ p = A $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce6b0b",
   "metadata": {},
   "source": [
    "#### the second one is\n",
    "$$ \\frac{\\partial}{\\partial b} C = -\\Sigma_v p_{target}(v) \\frac{ \\partial}{\\partial b} \\ln p(v) = -\\Sigma_v p_{target}(v) \\frac{e^{b + wv}}{1 + e^{b + wv}} = -\\Sigma_v p_{target}(v) \\frac{1}{1+e^{-(b+wv)}} = -\\frac{1-p}{1+e^{-b}} -\\frac{p}{1+e^{-(b+w)}} = 0$$\n",
    "\n",
    "$$ -\\frac{p}{1-p} = \\frac{ 1+e^{-(b+w)} }{1+e^{-b}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e3b5f",
   "metadata": {},
   "source": [
    "#### the third one is\n",
    "$$ \\frac{\\partial}{\\partial w} C =  -\\Sigma_v p_{target}(v) [\\frac{v e^{av + b + wv}}{ e^{av} + e^{av + b + wv}} - \\frac{e^{a+b+w}}{1 + e^b + e^a + e^{a+b+w}}] = (1-p) \\frac{e^{a+b+w}}{1 + e^b + e^a + e^{a+b+w}} -p [\\frac{e^{b + w}}{1 + e^{b + w}} - \\frac{e^{a+b+w}}{1 + e^b + e^a + e^{a+b+w}}] = \\frac{e^{a+b+w}}{1 + e^b + e^a + e^{a+b+w}} - p\\frac{e^{b + w}}{1 + e^{b + w}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a9d21",
   "metadata": {},
   "source": [
    "if we plug $p = \\frac{e^a (1+e^{b+w})}{1 + e^b + e^a + e^{a+b+w}}$ into $\\frac{e^{a+b+w}}{1 + e^b + e^a + e^{a+b+w}} - p\\frac{e^{b + w}}{1 + e^{b + w}} = 0$ then we get a trivial identity => these equations are dependent. \n",
    "let's take equations $$ -\\frac{p}{1-p} = \\frac{ 1+e^{-(b+w)} }{1+e^{-b}} $$ and  $$\\frac{e^{a+b+w}}{1 + e^b + e^a + e^{a+b+w}} - p\\frac{e^{b + w}}{1 + e^{b + w}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763684f",
   "metadata": {},
   "source": [
    "values of $b$ and $w$ that solve them are\n",
    "$$b = \\ln ( \\frac{1}{2}{  \\sqrt{(e^a+1)^2(p-1)^2} -p - e^2(p-1) -1  }) $$\n",
    "$$w = \\ln ( \\frac{ e^{-a}[ e^a(p-1)+\\sqrt{ (e^a+1)^2(p-1)^2 } - p +1 ] }{2(p-1)} ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9763035",
   "metadata": {},
   "source": [
    "it means there is an infinite number of tuples $(a,b,w)$ which minimize the cost function for a given probability distribution of a single visible neuron. we can fix value of $a$ and find $b$ and $w$ using this formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf63aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
